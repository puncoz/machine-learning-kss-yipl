{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AND-statement problem with a single perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before talking about perceptrons, lets begin with logic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](and-truth-table.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PLOT THE TRUTH TABLE\n",
    "![image.png](and-plot.png)\n",
    "\n",
    "In this graph, the point (1,1) — corresponding to x1 = 1 and x2 = 1, is a **filled-in circle**. A filled circle is one that has a truth value of 1 and in this case it represents the x1 ^ x2 statement. The point (0,0) on the other hand is empty because it represents a truth value of 0 for the x1 ^ x2 statement for which the inputs are x1 = 0 and x2 = 0.\n",
    "\n",
    "The line with the negative slope is some line, which bisects the input space in such a way that the points represented by empty circles (truth value of x1 ^ x2 = 0) are separated from points represented by the filled circle (truth value of x1 ^ x2 = 1). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Objectives\n",
    "\n",
    "To come up with some mathematical model which will be able to correctly identify whether a given point is an empty circle or a filled circle by separating the these two classes of points.\n",
    "\n",
    "#### So, how to find such a line that strategically separates these points though?\n",
    "Well, it is a straight line so we can represent it as some linear function (A) which we will call the **activity function**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ACTIVITY FUNCTION\n",
    "\n",
    "Let’s define an activity function A as some **weighted sum** of input variables x1 and x2 such that the function represents the line that serves as a boundary bisecting the input space.\n",
    "\n",
    "![image.png](act-fn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### THE PERCEPTRON\n",
    "We can model the truth value of the AND statement with a single perceptron in the following way:\n",
    "![image.png](perceptron-and.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s calculate a simple example with the following values of inputs, weights and bias:\n",
    "\n",
    "![image.png](and-calculation.png)\n",
    "\n",
    "In this example, the activity value is 0.5. What if we had chosen inputs x1 = 0 and x2 = 2 and kept the bias as theta = -1.5? In this case, the activity value would be a negative number.\n",
    "\n",
    "This is problematic because this does not output a binary value as we would have liked. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](perceptron-complex.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ACTIVATION FUNCTION\n",
    "![image.png](actication-fn.png)\n",
    "Such an activation function will map the activity value to a 1 if the activity value is greater than or equal to the threshold represented by a constant C, and to 0 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](sigmoid.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some list of activation function:\n",
    "https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Picture of Perceptron\n",
    "![image.png](final-perceptron.png)\n",
    "![image.png](neurons.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Single Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize the Weights and Calculate the Actual Output\n",
    "\n",
    "![image.png](learning1.png)\n",
    "\n",
    "The input x_i is multiplied by a randomly initialized weight w_ij and is fed into the perceptron along with a bias and all other weighted inputs. Inside the perceptron the activity function is applied to this weighted sum of inputs plus a bias and its value is then fed as an argument into the activation function f. The value of the activation function produces the output y_j of perceptron j."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calculate the Error [Error / Loss Function]\n",
    "\n",
    "![image.png](learning2.png)\n",
    "![image.png](learning2-1.png)\n",
    "We square error function to ensure that the error is always a positive number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gradient Descent — Updating the Weights to Further Reduce Error\n",
    "![image.png](learning3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Refereces\n",
    "\n",
    "- http://page.mi.fu-berlin.de/rojas/neural/index.html.html = chapter 3 and 4\n",
    "- https://towardsdatascience.com/what-the-hell-is-perceptron-626217814f53\n",
    "- https://towardsdatascience.com/perceptron-learning-algorithm-d5db0deab975\n",
    "- https://blog.usejournal.com/the-perceptron-the-building-block-of-neural-networks-5a428d3f451d\n",
    "- https://medium.com/@thomascountz/perceptrons-in-neural-networks-dc41f3e4c1b9\n",
    "- https://www.sciencedirect.com/topics/engineering/perceptron"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
